# Neural Network for Breast Cancer Classification 

This project demonstrates how to build, train, and evaluate a feedforward neural network for a binary classification task. The goal is to predict whether a breast cancer tumor is malignant or benign based on a set of features.

## Project Overview

This repository contains a Jupyter Notebook (`breast_cancer.ipynb`) that walks through the entire process of applying a neural network to the Breast Cancer Wisconsin dataset from Scikit-learn and a requirements.txt file wich contains all the dependencies.

### Methodology

The project follows these key steps:
1.  **Data Loading**: The Breast Cancer dataset is loaded directly from `sklearn.datasets`.
2.  **Preprocessing**: The data is split into training and testing sets to ensure an unbiased evaluation. The features are then scaled using `StandardScaler` to help the model train efficiently.
3.  **Model Building**: A sequential neural network is constructed using TensorFlow/Keras with two hidden layers and a final output layer for binary classification.
4.  **Training**: The model is trained on the preprocessed training data using the Adam optimizer and binary cross-entropy loss function.
5.  **Evaluation**: The trained model's performance is assessed on the unseen test data using key metrics like accuracy, precision, recall, and the F1-score. A confusion matrix is also generated to visualize the classification results.

### How to Run This Project

1.  **Prerequisites**: Make sure you have Python installed, along with the following libraries:
    *   numpy
    *   pandas
    *   scikit-learn
    *   tensorflow
    *   matplotlib
    *   seaborn
    *   jupyter

    You can install them using pip:
    ```bash
    pip install -r requirements.txt    
    ```

2.  **Execution**: Open and run the `breast_cancer.ipynb` file in a Jupyter environment. The notebook is self-contained and includes explanations for each step.

## Core Concepts Explained

#### 1. What is the role of feature scaling/normalization in training neural networks?
Feature scaling is all about putting your data on a level playing field. Imagine you're trying to predict a house price using the `number of rooms` (e.g., 2-5) and the `square footage` (e.g., 800-4000). Without scaling, the huge numbers for square footage would completely overshadow the small numbers for rooms, making the model incorrectly think square footage is thousands of times more important. Scaling brings all features to a similar range, ensuring they contribute fairly to the learning process.

#### 2. Why do we split data into training and testing sets?
We split data for the same reason you study for an exam. You use the textbook and notes (the **training set**) to learn the material. Then, you take a final exam with new questions (the **testing set**) to prove you actually understood the concepts, not just memorized the answers. This process checks if our model can make accurate predictions on new data it has never seen before, which is crucial for trusting it in the real world.

*   **Use Case:** A company builds a model to detect fraudulent credit card transactions. It trains the model on a huge history of past transactions (training set). To see if it's actually effective, they must test it on the most recent transactions that the model has never seen before (testing set).

#### 3. What is the purpose of activation functions like ReLU or Sigmoid?
An activation function acts as a "decision-maker" inside a neuron, deciding if the information it received is important enough to pass along to the next layer. Without them, a neural network could only learn simple, straight-line relationships. By introducing non-linearity, activation functions let the network learn incredibly complex patterns.

*   **Use Case:** In facial recognition, these functions allow the network to recognize non-linear shapes like the curve of an eye or a smile. The network then combines these learned patterns to identify a complete face.

#### 4. Why is binary cross-entropy commonly used as a loss function for classification?
The loss function is a scoring system for how wrong a model's predictions are. Binary cross-entropy is a particularly smart scoring system for yes/no classification tasks because it gives a small penalty for a prediction that is slightly off, but a huge penalty for a prediction that is both very wrong and very confident. This pushes the model to become more accurate and realistically confident.

*   **Use Case:** If a medical model is 99.9% sure a tumor is benign when it's actually cancerous, binary cross-entropy will issue a massive penalty, forcing the model to learn from that critical mistake.

#### 5. How does the optimizer (e.g., Adam) affect training compared to plain gradient descent?
An optimizer's job is to find the set of model parameters that results in the lowest error. Plain gradient descent is like being blindfolded in a hilly field and only taking small, fixed-size steps downhill; it's slow and you might get stuck in a small ditch. An optimizer like Adam is much smarter. It acts like a hiker who can change their step size—taking big leaps on gentle slopes and small, careful steps on steep parts—and has momentum to avoid getting stuck.

*   **Use Case:** This efficiency is essential for training huge models like GPT-3, where a simple optimizer would be too slow and would likely fail to find a good solution.

#### 6. What does the confusion matrix tell you beyond just accuracy?
Accuracy just gives you the total percentage of correct predictions, which can be dangerously misleading. A confusion matrix provides the full story by showing you *how* the model was wrong. It breaks down the results into correct predictions (True Positives, True Negatives) and, more importantly, the two types of errors (False Positives, False Negatives).

*   **Use Case:** For an airport security scanner, a 99% accuracy rate sounds great. But a confusion matrix might reveal that the 1% of errors are all "False Negatives"—failing to detect a weapon. This type of error is far more dangerous than a "False Positive" (flagging a safe item), and the matrix makes this critical distinction clear.

#### 7. How can increasing the number of hidden layers or neurons impact model performance?
You can think of a network's layers and neurons as its "brainpower." Adding more can help it learn more complex patterns, just like moving from basic math to calculus requires more mental capacity. However, giving a simple problem too much brainpower can lead to overthinking, where the model starts memorizing noise in the data instead of the underlying pattern (this is called overfitting).

*   **Use Case:** A model for a self-driving car needs a very deep and large network to understand complex road scenes, but a simple model to predict customer clicks on an ad would require a much smaller architecture.

#### 8. What are some signs that your model is overfitting the training data?
Overfitting happens when a model learns the training data *too* well, including its noise and quirks. It’s like a student who memorizes the exact answers to practice questions but fails the real exam because they can't handle slightly different problems. The biggest sign is a large gap between training and testing performance: the model gets a near-perfect score on the data it trained on, but its accuracy drops significantly on new, unseen data.

#### 9. Why do we evaluate using precision, recall, and F1-score instead of accuracy alone?
We use these metrics because accuracy alone can fail badly, especially when one class is much more common than the other. If you're screening for a rare disease that affects 1 in 1000 people, a model that always predicts "no disease" would be 99.9% accurate, but completely useless!
*   **Recall** asks: "Of all the people who were actually sick, how many did we correctly identify?"
*   **Precision** asks: "Of all the people we identified as sick, how many were actually sick?"
*   **F1-Score** is a single metric that balances the trade-off between precision and recall.

*   **Use Case:** For a spam email filter, high **recall** is needed to catch all spam. But high **precision** is even more critical to ensure that an important email (like a job offer) is never mistakenly flagged as spam.

#### 10. How would you improve the model if it performs poorly on the test set?
If your model is like a bad recipe, you can try several fixes. You could use better ingredients (**get more or cleaner data**). You could adjust the cooking time or temperature (**tune hyperparameters** like the learning rate). You could add a secret ingredient that brings out the flavors (**feature engineering**). You could also use techniques to prevent it from "burning," like **regularization**, which stops the model from overfitting. Finally, you might just need to try a completely new recipe (**a different model architecture**).